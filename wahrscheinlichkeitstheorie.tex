\section{\textbf{WAHRSCHEINLICHKEITSTHEORIE} - Ereignisse und Wahrscheinlichkeiten}
\subsection{Zufallsereignisse}
Der Ausgangspunkt jeder wahrscheinlichkeitstheoretischen Betrachtung ist ein Zufallsvorgang oder Zufallsexperiment (Würfeln, prüfen eines Werkstücks). Einen Zufallsvorgang zeichnet aus, dass es mehrere, \emph{sich gegenseitig ausschließende} Ergebnisse gibt, von dem genau eines Eintreten wird, wobei nicht bekannt ist welches. Die \emph{möglichen} Ergebnisse eines Zufallsvorgang werden mit $\omega$ und die Menge aller Ergebnisse d.h. der \emph{Ergebnisraum} werden als $\Omega$ bezeichnet. Beim Einmaligen Würfeln kann $\omega$  z.B. $\omega = \{2\}$ sein, während $\Omega = \{1, 2, 3, 4, 5, 6\}$ ist. Wenn man beim Würfeln eine 5 oder 6 benötigen um zu gewinnen, ist man daran interessiert, ob das
Ergebnis des Würfelwurfes in einer bestimmten Teilmenge von $\Omega$ liegt, konkret in der Menge $\{5, 6\}$. Diese Teilmengen nennt man \emph{(Zufalls-)Ereignisse} und man sagt, dass ein Ereignis $A \subset \Omega$ eintritt, wenn das Ergebnis $\omega$ des Zufallsvorgang ein Element von A is. Die einelementigen Teilmengen $\{\omega\}$ von $\Omega$ nennt man auch \emph{Elementareereignisse}.\\\\
\textbf{Mengenlehre und wahrscheinlichkeitstheoretische Interpretation}: \emph{Kommutativgesetz}: $A \cap  B = B \cap A$. \emph{Assoziativgesetz}: $(A \cap B ) \cap C = A \cap (B \cap C)$. \emph{Distributivgesetz}: $(A \cup B) \cap C = (A \cap C) \cup (A \cap C)$. \emph{De Morgansche Regel}: $\overline{(A \cap B)} = \bar{A} \cup \bar{B}$ (geht auch, wenn man $\cap$ und $\cup$ vertauscht)\\\\
\textbf{Ereignisfelder}: Die Menge aller Ergebnisse bezeichnet man als \emph{Ereignisfeld} $\mathcal{F}$. Wenn der Ergebnisraum $\Omega$ endlich oder abzählbar unendlich ist, ist die Potenzmenge $\mathcal{P}(\Omega)$ das üblicherweise betrachtete Ereignisfeld. Ist $\Omega$ überabzählbar können als Ereignisfelder nur $\sigma$-Algebren betrachtet werden. Diese sind definiert durch: $\Omega \in \mathcal{F}$,\,\,\,$A \in \mathcal{F} \Rightarrow \bar{A} \in \mathcal{F}$,\,\,\,abzählbar viele Ereignisse $A_1, A_2, ... \in \mathcal{F} \Rightarrow \cup_{i=1}^\infty A_i \in \mathcal{F}$

\subsection{Laplace-Wahrscheinlichkeiten und Kombinatorik}
\begin{wrapfigure}{r}{0.4\textwidth}
    \vspace{-6mm}
    \centering
    \includegraphics[width=0.4\textwidth]{images/wiederholung_kombinatorik.png}
    \caption{Kombinatorik Wiederholung}
    \vspace{-6mm}
    \label{fig:}
\end{wrapfigure}
\textbf{Laplace-Wahrscheinlichkeit}: $P(A) = \frac{\text{Anzahl der für $A$ günstigen Ergebnisse}}{\text{Anzahl aller möglichen Ergebnisse}} = \frac{|A|}{|\Omega|}$. Bsp. die Wahrscheinlichkeit eine ungerade Zahl zu würfeln: $P(A) = \frac{|\{1,3,5\}|}{|\{1,2,3,4,5,6\}|} = \frac{3}{6} = \frac{1}{2}$

\textbf{Permutation}: Anordnen von n unterscheidbaren Objekten: $n \cdot (n-1) \cdot (n-2) ... = \Pi_{i=1}^n i = n!$. Bsp: Sitzordnung an einem Tisch.\,\,\,\,\textbf{Variation ohne Wiederholungen}: Möglichkeiten k aus n unterscheidbaren Objekten \emph{mit} Berücksichtigung der Reihenfolge auszuwählen. Formel 1 Siegertreppchen Anordnungen bei 20 Teilnehmern.\,\,\,\,\textbf{Variation mit Wiederholungen}: Bsp. k mal ziehen aus n Möglichkeiten. Bsp: Kombinationen bei Passwörtern.\,\,\,\,\textbf{Kombination ohne Wiederholungen} Ziehen von k aus n Objekten. Die Reihenfolge spielt keine Rolle. Bsp: \hlc{Lotto} 6 aus 49.\,\,\,\,\textbf{Kombination mit Wiederholungen}: Die Anzahl Kombinationen beim Auswählen von k Objekten aus n unterscheidbaren Objekten.
\subsection{Wahrscheinlichkeitsmaße}
\textbf{Definition Wahrscheinlichkeitsmaß}: Es sei $\Omega$ der Ergebnisraum eines Zufallsvorgangs und $\mathcal{F}$ ein Ereignisfeld über $\Omega$. Eine Abbildung $P$ die jedem Ereignis $A \in \mathcal{F}$ eine reelle Zahl zuordnet: $P: \mathcal{F} \rightarrow \mathds{R}$ mit $A \mapsto P(A)$ nennt man \emph{Wahrscheinlichkeitsmaß} auf $\mathcal{F}$ wenn sie die Axiome von Kolmogoroff erfüllt: \textbf{K1} $P(A) \ge 0$ (Nichtnegativität).\,\,\,\,\textbf{K2} $P(\Omega) = 1$ (Normierung).\,\,\,\,\textbf{K3} Falls $A \cap B = \emptyset$, so ist $ (A \cup B) = P(A) + P(B)$ (Additivität).\\
Daraus folgen folgende Rechenregel:\,\,\,\,(\textbf{i}) $0 \le P(A) \le 1$.\,\,\,\,(\textbf{ii}) $P(\emptyset) = 0$.\,\,\,\,(\textbf{iii}) $P(A) \le P(B)$, falls $A \subset B$.\,\,\,\,(\textbf{iv}) $P(\bar{A}) = 1 - P(A)$.\,\,\,\,(\textbf{v}) $P(A_1\cup ... \cup A_k) = P(A_1) + ... + P(A_k)$ falls $A_1, ..., A_k$ paarweise disjunkt. $\Rightarrow$ $P(A) = \sum_{w \in A}P(\{\omega\}.$\,\,\,\,(\textbf{vi}) $P(B \backslash A) = P(B) - P(A \cap B)$.\,\,\,\,(\textbf{vii}) $P(A\cup B) = P(A) + P(B) - P(A \cap B)$ (Additionssatz)
\subsection{Bedingte Wahrscheinlichkeiten und unabhängige Ereignisse}
Die Wahrscheinlichkeit eines Ereignisses ändert sich möglicherweise, wenn wir zusätzlich Information in Form des Eintritts eines anderen Ereignisses erhalten. Wahrscheinlichkeit für Pünktlichkeit sinkt bei Stau. Die Definition der \hlc{bedingten Wahrscheinlichkeit} ergibt sich hierbei Analog zur relativen Häufigkeitsverteilung: \hlcm{$P(A|B) = \frac{P(A \cap B)}{P(B)}$}. Bsp: Wie hoch ist die Wahrscheinlichkeit eine 6 zu würfeln unter der Bedingung, dass die gewürfelte Zahl gerade ist? Antwort: $P(A|B) = \frac{P(\{6\} \cap \{2,4,6\})}{P(\{2,4,6\})} = \frac{1/6}{3/6} = \frac{1}{3}$\\\\
\begin{wrapfigure}{l}{0.25\textwidth}
    \vspace{-8mm}
    \centering
    \includegraphics[width=0.25\textwidth]{images/beispiel_wahl_satz_der_tot_wahscheinlichkeit.png}
    \caption{Eine Partei hat analysiert, wie ihre Demographie ist.}
    \vspace{-5mm}
    \label{fig:pool_total}
\end{wrapfigure}
Der \hlc{Produktsatz} garantiert, dass für zwei Ergebnisse $A$ und $B$ mit $P(B) > 0$ gilt: \hlcm{$P(A \cap B) = P(B) \cdot P(A|B)$}. Diese Aussage gilt auch im allgemeinen für $A_1, ..., A_n$ mit $P(A_1 \cap ... \cap A_n) > 0$:\\$P(\cap_{i=1}^nA_i) = P(A_1) \cdot P(A_2|A_1) \cdot P(A_3|A_1 \cap A_2) \cdot P(A_n|A_1 \cap ... \cap A_{n-1}) = P(A_1) \cdot \Pi_{i=2}^n P(A_i|A_1 \cap ... \cap A_{i-1})$. \hlc{Stochastiche Unabhängikeit} bedeutet wenn sich die Wahrscheinlichkeit eines Ereignisses $A$ beim Eintritt eines anderen Ereignisses $B$ \emph{nicht} ändert. Sprich $P(A|B) = P(A)$. Somit gild dann mit dem Produktsatz $P(A \cap B) = P(B) \cdot P(A | B) = P(B) \cdot P(A)$. Dies gilt Analog für $n$ Ereignisse. \hlc{Satz der totalen Wahrscheinlichkeit}: Sei $A_1, ..., A_n$ eine disjunkte Zerlegung des Ereignisraums $\omega$ mit $P(A_j) > 0$, $j = 1, ..., n$. Dann gilt für ein weiteres Ereignis $B$: \hlcm{$P(B) = \sum_{j=1}^nP(B|A_j) \cdot P(A_j)$}. Bsp Wahl (vgl \cref{fig:pool_total}) Welchen Stimmanteil hat die Partei insgesamt bekommen? Antwort: $P(B) = \sum_{j=1}^3 = P(B|A_j) \cdot P(A_j) = 0.28 \cdot 0.27 + 0.03 \cdot 0.031 + 0.04 \cdot 0.42 = 0.1017$. \hlc{Satz von Bayes}: Sei $A_1, ..., A_n$ eine disjunkte Überdeckung des Ereignisses $B$ und $P(A_j) > 0$ sowie $P(B|A_j) > 0$ für mindestens ein $j = 1, ..., n$. Dann gilt: \hlcm{$P(A_j|B) = \frac{P(B|A_j) \cdot P(A_j)}{P(B)} = \frac{P(B|A_j) \cdot P(A_j)}{\sum_{i=1}^nP(B|A_i) \cdot P(A_i)}, j = 1, ..., n$}. Dieser Satz überträgt die \emph{a-priori} Wahrscheinlichkeit $P(A_j)$ in eine \emph{a-posteriori} Wahrscheinlichkeit $P(A_j|B)$. Bsp. Wie groß ist die Wahrscheinlichkeit, dass ein Wähler zur Gruppe \glqq jünger als 40\grqq\, gehört? Antwort: ($P(B)$ wurde mit Satz der tot. Wahrscheinlichkeit berechnet) $P(A_1|B) = \frac{P(B|A_1) \cdot P(A_1)}{P(B)} = \frac{0.28 \cdot 0.27}{0.1017}$
\section{Diskrete Zufallsvariablen}
\section{Stetige Zufallsvariablen}
\section{Gesetz der großen Zahlen und zentraler Grenzwertsatz}
\section{Zufallsvektoren}
