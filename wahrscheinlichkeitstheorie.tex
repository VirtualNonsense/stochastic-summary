\section{\textbf{WAHRSCHEINLICHKEITSTHEORIE} - Ereignisse und Wahrscheinlichkeiten}
\subsection{Zufallsereignisse}
Der Ausgangspunkt jeder wahrscheinlichkeitstheoretischen Betrachtung ist ein Zufallsvorgang oder Zufallsexperiment (Würfeln, prüfen eines Werkstücks). Einen Zufallsvorgang zeichnet aus, dass es mehrere, \emph{sich gegenseitig ausschließende} Ergebnisse gibt, von dem genau eines Eintreten wird, wobei nicht bekannt ist welches. Die \emph{möglichen} Ergebnisse eines Zufallsvorgang werden mit $\omega$ und die Menge aller Ergebnisse d.h. der \emph{Ergebnisraum} werden als $\Omega$ bezeichnet. Beim Einmaligen Würfeln kann $\omega$  z.B. $\omega = \{2\}$ sein, während $\Omega = \{1, 2, 3, 4, 5, 6\}$ ist. Wenn man beim Würfeln eine 5 oder 6 benötigen um zu gewinnen, ist man daran interessiert, ob das
Ergebnis des Würfelwurfes in einer bestimmten Teilmenge von $\Omega$ liegt, konkret in der Menge $\{5, 6\}$. Diese Teilmengen nennt man \emph{(Zufalls-)Ereignisse} und man sagt, dass ein Ereignis $A \subset \Omega$ eintritt, wenn das Ergebnis $\omega$ des Zufallsvorgang ein Element von A is. Die einelementigen Teilmengen $\{\omega\}$ von $\Omega$ nennt man auch \emph{Elementareereignisse}.\\\\
\textbf{Mengenlehre und wahrscheinlichkeitstheoretische Interpretation}: \emph{Kommutativgesetz}: $A \cap  B = B \cap A$. \emph{Assoziativgesetz}: $(A \cap B ) \cap C = A \cap (B \cap C)$. \emph{Distributivgesetz}: $(A \cup B) \cap C = (A \cap C) \cup (A \cap C)$. \emph{De Morgansche Regel}: $\overline{(A \cap B)} = \bar{A} \cup \bar{B}$ (geht auch, wenn man $\cap$ und $\cup$ vertauscht)\\\\
\textbf{Ereignisfelder}: Die Menge aller Ergebnisse bezeichnet man als \emph{Ereignisfeld} $\mathcal{F}$. Wenn der Ergebnisraum $\Omega$ endlich oder abzählbar unendlich ist, ist die Potenzmenge $\mathcal{P}(\Omega)$ das üblicherweise betrachtete Ereignisfeld. Ist $\Omega$ überabzählbar können als Ereignisfelder nur $\sigma$-Algebren betrachtet werden. Diese sind definiert durch: $\Omega \in \mathcal{F}$,\,\,\,$A \in \mathcal{F} \Rightarrow \bar{A} \in \mathcal{F}$,\,\,\,abzählbar viele Ereignisse $A_1, A_2, ... \in \mathcal{F} \Rightarrow \cup_{i=1}^\infty A_i \in \mathcal{F}$

\subsection{Laplace-Wahrscheinlichkeiten und Kombinatorik}
\begin{wrapfigure}{r}{0.4\textwidth}
    \vspace{-6mm}
    \centering
    \includegraphics[width=0.4\textwidth]{images/wiederholung_kombinatorik.png}
    \caption{Kombinatorik Wiederholung}
    \vspace{-6mm}
    \label{fig:}
\end{wrapfigure}
\textbf{Laplace-Wahrscheinlichkeit}: $P(A) = \frac{\text{Anzahl der für $A$ günstigen Ergebnisse}}{\text{Anzahl aller möglichen Ergebnisse}} = \frac{|A|}{|\Omega|}$. Bsp. die Wahrscheinlichkeit eine ungerade Zahl zu würfeln: $P(A) = \frac{|\{1,3,5\}|}{|\{1,2,3,4,5,6\}|} = \frac{3}{6} = \frac{1}{2}$

\textbf{Permutation}: Anordnen von n unterscheidbaren Objekten: $n \cdot (n-1) \cdot (n-2) ... = \Pi_{i=1}^n i = n!$. Bsp: Sitzordnung an einem Tisch.\,\,\,\,\textbf{Variation ohne Wiederholungen}: Möglichkeiten k aus n unterscheidbaren Objekten \emph{mit} Berücksichtigung der Reihenfolge auszuwählen. Formel 1 Siegertreppchen Anordnungen bei 20 Teilnehmern.\,\,\,\,\textbf{Variation mit Wiederholungen}: Bsp. k mal ziehen aus n Möglichkeiten. Bsp: Kombinationen bei Passwörtern.\,\,\,\,\textbf{Kombination ohne Wiederholungen} Ziehen von k aus n Objekten. Die Reihenfolge spielt keine Rolle. Bsp: \hlc{Lotto} 6 aus 49.\,\,\,\,\textbf{Kombination mit Wiederholungen}: Die Anzahl Kombinationen beim Auswählen von k Objekten aus n unterscheidbaren Objekten.
\subsection{Wahrscheinlichkeitsmaße}
\textbf{Definition Wahrscheinlichkeitsmaß}: Es sei $\Omega$ der Ergebnisraum eines Zufallsvorgangs und $\mathcal{F}$ ein Ereignisfeld über $\Omega$. Eine Abbildung $P$ die jedem Ereignis $A \in \mathcal{F}$ eine reelle Zahl zuordnet: $P: \mathcal{F} \rightarrow \mathds{R}$ mit $A \mapsto P(A)$ nennt man \emph{Wahrscheinlichkeitsmaß} auf $\mathcal{F}$ wenn sie die \hlc{Axiome von Kolmogoroff} erfüllt: \textbf{K1} $P(A) \ge 0$ (Nichtnegativität).\,\,\,\,\textbf{K2} $P(\Omega) = 1$ (Normierung).\,\,\,\,\textbf{K3} Falls $A \cap B = \emptyset$, so ist $ (A \cup B) = P(A) + P(B)$ (Additivität).\\
Daraus folgen folgende Rechenregel:\,\,\,\,(\textbf{i}) $0 \le P(A) \le 1$.\,\,\,\,(\textbf{ii}) $P(\emptyset) = 0$.\,\,\,\,(\textbf{iii}) $P(A) \le P(B)$, falls $A \subset B$.\,\,\,\,(\textbf{iv}) $P(\bar{A}) = 1 - P(A)$.\,\,\,\,(\textbf{v}) $P(A_1\cup ... \cup A_k) = P(A_1) + ... + P(A_k)$ falls $A_1, ..., A_k$ paarweise disjunkt. $\Rightarrow$ $P(A) = \sum_{w \in A}P(\{\omega\}.$\,\,\,\,(\textbf{vi}) $P(B \backslash A) = P(B) - P(A \cap B)$.\,\,\,\,(\textbf{vii}) $P(A\cup B) = P(A) + P(B) - P(A \cap B)$ (Additionssatz)
\subsection{Bedingte Wahrscheinlichkeiten und unabhängige Ereignisse}
Die Wahrscheinlichkeit eines Ereignisses ändert sich möglicherweise, wenn wir zusätzlich Information in Form des Eintritts eines anderen Ereignisses erhalten. Wahrscheinlichkeit für Pünktlichkeit sinkt bei Stau. Die Definition der \hlc{bedingten Wahrscheinlichkeit} ergibt sich hierbei Analog zur relativen Häufigkeitsverteilung: \hlcm{$P(A|B) = \frac{P(A \cap B)}{P(B)}$}. Bsp: Wie hoch ist die Wahrscheinlichkeit eine 6 zu würfeln unter der Bedingung, dass die gewürfelte Zahl gerade ist? Antwort: $P(A|B) = \frac{P(\{6\} \cap \{2,4,6\})}{P(\{2,4,6\})} = \frac{1/6}{3/6} = \frac{1}{3}$\\
\begin{wrapfigure}{l}{0.25\textwidth}
    \vspace{-6mm}
    \centering
    \includegraphics[width=0.25\textwidth]{images/beispiel_wahl_satz_der_tot_wahscheinlichkeit.png}
    \caption{Eine Partei hat analysiert, wie ihre Demographie ist.}
    \vspace{-5mm}
    \label{fig:pool_total}
\end{wrapfigure}
Der \hlc{Produktsatz} garantiert, dass für zwei Ergebnisse $A$ und $B$ mit $P(B) > 0$ gilt: \hlcm{$P(A \cap B) = P(B) \cdot P(A|B)$}. Diese Aussage gilt auch im allgemeinen für $A_1, ..., A_n$ mit $P(A_1 \cap ... \cap A_n) > 0$:\\$P(\cap_{i=1}^nA_i) = P(A_1) \cdot P(A_2|A_1) \cdot P(A_3|A_1 \cap A_2) \cdot P(A_n|A_1 \cap ... \cap A_{n-1}) = P(A_1) \cdot \Pi_{i=2}^n P(A_i|A_1 \cap ... \cap A_{i-1})$. \hlc{Stochastiche Unabhängikeit} bedeutet wenn sich die Wahrscheinlichkeit eines Ereignisses $A$ beim Eintritt eines anderen Ereignisses $B$ \emph{nicht} ändert. Sprich $P(A|B) = P(A)$. Somit gild dann mit dem Produktsatz $P(A \cap B) = P(B) \cdot P(A | B) = P(B) \cdot P(A)$. Dies gilt Analog für $n$ Ereignisse. \hlc{Satz der totalen Wahrscheinlichkeit}: Sei $A_1, ..., A_n$ eine disjunkte Zerlegung des Ereignisraums $\omega$ mit $P(A_j) > 0$, $j = 1, ..., n$. Dann gilt für ein weiteres Ereignis $B$: \hlcm{$P(B) = \sum_{j=1}^nP(B|A_j) \cdot P(A_j)$}. Bsp Wahl (vgl \cref{fig:pool_total}) Welchen Stimmanteil hat die Partei insgesamt bekommen? Antwort: $P(B) = \sum_{j=1}^3 = P(B|A_j) \cdot P(A_j) = 0.28 \cdot 0.27 + 0.03 \cdot 0.031 + 0.04 \cdot 0.42 = 0.1017$. \hlc{Satz von Bayes}: Sei $A_1, ..., A_n$ eine disjunkte Überdeckung des Ereignisses $B$ und $P(A_j) > 0$ sowie $P(B|A_j) > 0$ für mindestens ein $j = 1, ..., n$. Dann gilt: \hlcm{$P(A_j|B) = \frac{P(B|A_j) \cdot P(A_j)}{P(B)} = \frac{P(B|A_j) \cdot P(A_j)}{\sum_{i=1}^nP(B|A_i) \cdot P(A_i)}, j = 1, ..., n$}. Dieser Satz überträgt die \emph{a-priori} Wahrscheinlichkeit $P(A_j)$ in eine \emph{a-posteriori} Wahrscheinlichkeit $P(A_j|B)$. Bsp. Wie groß ist die Wahrscheinlichkeit, dass ein Wähler zur Gruppe \glqq jünger als 40\grqq\, gehört? Antwort: ($P(B)$ wurde mit Satz der tot. Wahrscheinlichkeit berechnet) $P(A_1|B) = \frac{P(B|A_1) \cdot P(A_1)}{P(B)} = \frac{0.28 \cdot 0.27}{0.1017}$
\section{Zufallsvariablen}
\begin{wrapfigure}{r}{0.2\textwidth}
    \vspace{-10mm}
    \centering
    \includegraphics[width=0.2\textwidth]{images/5.1_f_augensumme.png}
    \includegraphics[width=0.2\textwidth]{images/5.2_F_augensumme.png}
    \caption{Wahrscheinlichkeitsion (oben) und Verteilungsfunktion für die Augensummen bei zweimaligen Würfeln.}
    \vspace{-3mm}
    \label{fig:f_F_dice}
\end{wrapfigure}
Bei der Beobachtung oder Durchführung von Zufallsvorgängen ist man häufig gar nicht an den Ergebnissen selbst interessiert, sondern an daraus abgeleiteten Größen. Beim zweimaligen Würfeln könnte dies beispielweise die Summe der Augenzahlen sein. Hier besteht der Ereignisraum $\Omega$ aus 36 Zahlenpaaren $\omega = (i, j)$ mit $1 \le i,j \le 6$, relevant ist für jedes Zahlenpaar aber nur die Information $i + j$. Durch die Zuordnung der reellen Zahl $i + j$ zu jedem Ergebnis $\omega = (i, j)$ erhält man eine \emph{Zufallsvariable}. Wenn wir $X$ für die Augensumme der zwei Würfe verwenden erhalten wir also die Zuordnung bzw. Abbildung $X: \omega = (i, j) X(\omega) = X(i, j) = i + j$. Diese Schreibweise ermöglicht es Ereignisse sehr knapp und einfach zu Formulieren. So entspricht dem Ereignis \glqq die Augensumme ist 4\grqq\, kurz \{$X=4$\} was ausführlich $\{X = 4\} = \{\omega \in \Omega: X(\omega) = 4 = \{(1,3), (2,2), (3,1)\}$ bedeutet. 
\section{Diskrete Zufallsvariablen}
Eine Zufallsvariable heißt diskret, wenn sie nur endlich oder abzählbar unendlich viele Werte annehmen kann, d.h. $W(X) = \{x_1, x_2, ...\}$. Die Wahrscheinlichkeitsverteilung ist dann gegeben durch $P(X = x_i) = p_i$ eindeutig festgelegt. Nach dem ersten Axiom von Kolmogoroff ist $0 \le p_i \le 1$ und die Wahrscheinlichkeit für ein Ereignis $\{X \in A\}$ mit $A \subset W(X)$ ist $P(X \in A) = \sum_{i:x_i \in A} p_i$. Nach dem zweiten Axiom von Kolmogoroff folgt damit sofort $\sum_{i \ge 1} p_i = P(X \in W(X)) = 1$. Die Wahrscheinlichkeitsverteilung ist Analog zur relativen Häufigkeitsverteilung. Während die Wahrscheinlichkeitsverteilung die möglichen Ausgänge eines Zufallsvorgangs theoretisch beschreibt basiert die relative Häufigkeitsverteilung auf tatsächlichen Beobachtungen in einer Stichprobe.
\hlc{Definition Wahrscheinlichkeitsfunktion.}: \hlcm{$f(x) = f_X() := \begin{cases}
    P(X = x_i) = p_i &\text{, }x = x_i \in W(X)\\
    0 &\text{, sonst.}
\end{cases}$}. $W(X)$ sei hierbei der Wertebereich einer Zufallsvariable X.
Die \hlc{Verteilungsfunktion} ist \hlcm{$F(x) = F_X(x) := P(X\le x) = \sum_{i: x_i \le x}f(x_i) = \sum_{i: x_i \le x} p_i$}. Die Verteilungsfunktion ist insbesondere zur \hlc{Berechnung von Wahrscheinlichkeiten der Form $P(a < X \le b)$} mit $a, b \in \mathds{R}$ und $a < b$ hilfreich, denn es gilt \hlcm{$P(a < X \le b) = F(b) - F(a)$}\\\\
\hlc{Unabhänigkeit diskreter Zufallsvariablen}: $n$ Diskrete Zufallsvariablen $X_1, .., X_n$ heißen stochastisch unabhängig, wenn für beliebige $x_1 \in W(X_1), ..., x_n \in W(X_n)$ gilt: $P(X_1 = x_1, ..., X_n = x_n) = P(X_1 = x_1) \cdot ... \cdot P(X_n = x_n)$. Anschaulich: Für das Würfeln von zwei Würfeln  $X_1$ und $X_2$ mit zwei beliebigen Augenzahlen $i$ und $j$ gilt: $P(X_1 = i, X_2 = j) = \frac{1}{36} = \frac{1 \cdot 1}{6 \cdot 6} = P(X_1 = i) \cdot P(X_2 = j)$.

\subsection{Maßzahlen diskreter Wahrscheinlich}
\section{Stetige Zufallsvariablen}
\section{Gesetz der großen Zahlen und zentraler Grenzwertsatz}
\section{Zufallsvektoren}
